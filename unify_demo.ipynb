{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import tqdm as notebook_tqdm\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "import dsp\n",
        "import dspy\n",
        "from dotdict import dotdict\n",
        "from dspy.datasets.gsm8k import GSM8K, gsm8k_metric\n",
        "from dspy.evaluate import Evaluate\n",
        "from dspy.teleprompt import BootstrapFewShot, BootstrapFinetune\n",
        "\n",
        "load_dotenv()\n",
        "unify_api_key = os.getenv(\"UNIFY_KEY\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(20, 50)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dspy.datasets import HotPotQA\n",
        "\n",
        "# Load the dataset.\n",
        "dataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=0)\n",
        "\n",
        "# Tell DSPy that the 'question' field is the input. Any other fields are labels and/or metadata.\n",
        "trainset = [x.with_inputs('question') for x in dataset.train]\n",
        "devset = [x.with_inputs('question') for x in dataset.dev]\n",
        "\n",
        "len(trainset), len(devset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GenerateAnswer(dspy.Signature):\n",
        "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
        "\n",
        "    question = dspy.InputField()\n",
        "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n",
        "\n",
        "class QnA(dspy.Module):\n",
        "    def __init__(self, num_passages=3):\n",
        "        super().__init__()\n",
        "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
        "\n",
        "    def forward(self, question):\n",
        "        prediction = self.generate_answer(question=question)\n",
        "        return dspy.Prediction(answer=prediction.answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "gemma = dsp.Unify(\n",
        "    endpoint=\"gemma-7b-it@anyscale\",\n",
        "    max_tokens=150,\n",
        "    api_key=unify_api_key,\n",
        ")\n",
        "\n",
        "claude = dsp.Unify(\n",
        "    endpoint=\"claude-3-haiku@anthropic\",\n",
        "    max_tokens=150,\n",
        "    api_key=unify_api_key,\n",
        ")\n",
        "\n",
        "unify_router = dsp.Unify(\n",
        "    max_tokens=150,\n",
        "    api_key=unify_api_key,\n",
        ")\n",
        "\n",
        "\n",
        "dspy.settings.configure(lm=gemma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]\n",
            "  5%|\u258c         | 1/20 [00:09<02:59,  9.44s/it]\n",
            " 10%|\u2588         | 2/20 [00:18<02:48,  9.38s/it]"
          ]
        }
      ],
      "source": [
        "def validate_context_and_answer(example, pred, trace=None):\n",
        "    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n",
        "    return answer_EM\n",
        "\n",
        "num_threads = 24\n",
        "\n",
        "teleprompter = BootstrapFewShot(metric=validate_context_and_answer)\n",
        "\n",
        "compiled_qna = teleprompter.compile(QnA(), trainset=trainset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:dspy.evaluate.evaluate:2024-07-16T17:37:00.069301Z [error    ] Error for example in dev set: \t\t Unify.__call__() missing 1 required positional argument: 'prompt' [dspy.evaluate.evaluate] filename=evaluate.py lineno=180\n",
            "ERROR:dspy.evaluate.evaluate:2024-07-16T17:37:00.113339Z [error    ] Error for example in dev set: \t\t Unify.__call__() missing 1 required positional argument: 'prompt' [dspy.evaluate.evaluate] filename=evaluate.py lineno=180\n",
            "ERROR:dspy.evaluate.evaluate:2024-07-16T17:37:00.108311Z [error    ] Error for example in dev set: \t\t Unify.__call__() missing 1 required positional argument: 'prompt' [dspy.evaluate.evaluate] filename=evaluate.py lineno=180\n",
            "ERROR:dspy.evaluate.evaluate:2024-07-16T17:37:00.120303Z [error    ] Error for example in dev set: \t\t Unify.__call__() missing 1 required positional argument: 'prompt' [dspy.evaluate.evaluate] filename=evaluate.py lineno=180\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Unify.__call__() missing 1 required positional argument: 'prompt'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m evaluate_hotpot \u001b[38;5;241m=\u001b[39m Evaluate(devset\u001b[38;5;241m=\u001b[39mdevset[:\u001b[38;5;241m1000\u001b[39m], metric\u001b[38;5;241m=\u001b[39mvalidate_context_and_answer, num_threads\u001b[38;5;241m=\u001b[39mnum_threads, display_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, display_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mevaluate_hotpot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgemma\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Dokumenty\\Python\\dspy\\dspy\\evaluate\\evaluate.py:193\u001b[0m, in \u001b[0;36mEvaluate.__call__\u001b[1;34m(self, program, metric, devset, num_threads, display_progress, display_table, return_all_scores, return_outputs)\u001b[0m\n\u001b[0;32m    191\u001b[0m     reordered_devset, ncorrect, ntotal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_single_thread(wrapped_program, devset, display_progress)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     reordered_devset, ncorrect, ntotal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_multi_thread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrapped_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplay_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m dspy\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Metric: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mncorrect\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mntotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mncorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mntotal,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    202\u001b[0m predicted_devset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(reordered_devset)\n",
            "File \u001b[1;32md:\\Dokumenty\\Python\\dspy\\dspy\\evaluate\\evaluate.py:110\u001b[0m, in \u001b[0;36mEvaluate._execute_multi_thread\u001b[1;34m(self, wrapped_program, devset, num_threads, display_progress)\u001b[0m\n\u001b[0;32m    107\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(devset), dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m display_progress)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m as_completed(futures):\n\u001b[1;32m--> 110\u001b[0m     example_idx, example, prediction, score \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# use the cancelled_job literal to check if the job was cancelled - use \"is\" not \"==\"\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# in case the prediction is \"cancelled\" for some reason.\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prediction \u001b[38;5;129;01mis\u001b[39;00m job_cancelled:\n",
            "File \u001b[1;32mD:\\Users\\U\u017cytkownik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
            "File \u001b[1;32mD:\\Users\\U\u017cytkownik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mD:\\Users\\U\u017cytkownik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
            "File \u001b[1;32md:\\Dokumenty\\Python\\dspy\\dspy\\evaluate\\evaluate.py:103\u001b[0m, in \u001b[0;36mEvaluate._execute_multi_thread.<locals>.cancellable_wrapped_program\u001b[1;34m(idx, arg)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_jobs\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, job_cancelled, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_program\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Dokumenty\\Python\\dspy\\dspy\\evaluate\\evaluate.py:178\u001b[0m, in \u001b[0;36mEvaluate.__call__.<locals>.wrapped_program\u001b[1;34m(example_idx, example)\u001b[0m\n\u001b[0;32m    176\u001b[0m     current_error_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_count\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_error_count \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_errors:\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    180\u001b[0m dspy\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError for example in dev set: \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m example_idx, example, {}, \u001b[38;5;241m0.0\u001b[39m\n",
            "File \u001b[1;32md:\\Dokumenty\\Python\\dspy\\dspy\\evaluate\\evaluate.py:160\u001b[0m, in \u001b[0;36mEvaluate.__call__.<locals>.wrapped_program\u001b[1;34m(example_idx, example)\u001b[0m\n\u001b[0;32m    157\u001b[0m     thread_stacks[threading\u001b[38;5;241m.\u001b[39mget_ident()] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(dspy\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mmain_stack)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mprogram\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m     score \u001b[38;5;241m=\u001b[39m metric(\n\u001b[0;32m    162\u001b[0m         example,\n\u001b[0;32m    163\u001b[0m         prediction,\n\u001b[0;32m    164\u001b[0m     )  \u001b[38;5;66;03m# FIXME: TODO: What's the right order? Maybe force name-based kwargs!\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# increment assert and suggest failures to program's attributes\u001b[39;00m\n",
            "\u001b[1;31mTypeError\u001b[0m: Unify.__call__() missing 1 required positional argument: 'prompt'"
          ]
        }
      ],
      "source": [
        "evaluate_hotpot = Evaluate(devset=devset[:1000], metric=validate_context_and_answer, num_threads=num_threads, display_progress=True, display_table=0)\n",
        "evaluate_hotpot(compiled_qna)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
